{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64 # How many experiences to use for each training step.\n",
    "update_freq = 8 # How often to perform a training step.\n",
    "y = 0.8 # Discount factor on the target Q-values\n",
    "startE = 1 # Starting chance of random action\n",
    "endE = 0.1 # Final chance of random action\n",
    "anneling_steps = 100000 # How many steps of training to reduce startE to endE.\n",
    "num_episodes = 5000 # How many episodes of game environment to train network with.\n",
    "pre_train_steps = 5000 # How many steps of random actions before training begins.\n",
    "model_path = \"./models/dqn-12\" # The path to save our model to.\n",
    "summary_path = './summaries/dqn-12' # The path to save summary statistics to.\n",
    "h_size = 128 # The number of units in the hidden layer.\n",
    "learning_rate = 3e-4 # Agent Learning Rate\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Unity environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'GridAcademy' started successfully!\n",
      "Unity Academy name: GridAcademy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgridSize -> 5.0\n",
      "\t\tnumObstacles -> 1.0\n",
      "\t\tnumGoals -> 1.0\n",
      "Unity brain name: GridWorldBrain\n",
      "        Number of Visual Observations (per agent): 1\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 0\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./envs/GridWorld\", worker_id=4)\n",
    "default_brain = env.brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent state looks like: \n",
      "[]\n",
      "Agent observations look like:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ee002b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset the environment\n",
    "env_info = env.reset()[default_brain]\n",
    "\n",
    "# Examine the state space for the default brain\n",
    "print(\"Agent state looks like: \\n{}\".format(env_info.vector_observations[0]))\n",
    "\n",
    "# Examine the observation space for the default brain\n",
    "for observation in env_info.visual_observations:\n",
    "    print(\"Agent observations look like:\")\n",
    "    if observation.shape[3] == 3:\n",
    "        plt.imshow(observation[0,:,:,:])\n",
    "    else:\n",
    "        plt.imshow(observation[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.009999999776482582] [False]\n",
      "Agent observations look like:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ee43ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "brain = env.step(2)[default_brain]\n",
    "print(brain.rewards, brain.local_done)\n",
    "for observation in brain.visual_observations:\n",
    "    print(\"Agent observations look like:\")\n",
    "    if observation.shape[3] == 3:\n",
    "        plt.imshow(observation[0,:,:,:])\n",
    "    else:\n",
    "        plt.imshow(observation[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation (o) corresponds to a pixel image of the screen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the network itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self ,h_size, num_actions, lr, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            # The network recieves a frame from the game, flattened into an array.\n",
    "            # It then resizes it and processes it through four convolutional layers.\n",
    "            self.observation_input =  tf.placeholder(shape=[None, 32, 32, 3],dtype=tf.float32)\n",
    "            self.conv1 = tf.layers.conv2d(self.observation_input, 16, \n",
    "                                     kernel_size=[3, 3], strides=[2, 2], \n",
    "                                     use_bias=False,\n",
    "                                     activation=tf.nn.elu)\n",
    "            self.conv2 = tf.layers.conv2d(self.conv1, 32, \n",
    "                                     use_bias=False,\n",
    "                                     kernel_size=[3, 3], \n",
    "                                     strides=[2, 2], \n",
    "                                     activation=tf.nn.elu)\n",
    "\n",
    "            # We take the output from the final convolutional layer \n",
    "            # and split it into separate advantage and value streams.\n",
    "            self.hidden = tf.layers.dense(tf.layers.flatten(self.conv2), \n",
    "                                               h_size, activation=tf.nn.elu)\n",
    "            self.Q_estimate = tf.layers.dense(self.hidden, num_actions, activation=None,\n",
    "                                                  use_bias=False)\n",
    "\n",
    "            self.predict = tf.argmax(self.Q_estimate, 1)\n",
    "            self.value = tf.reduce_mean(self.Q_estimate, axis=1)\n",
    "\n",
    "            # Below we obtain the loss by taking the sum of squares difference \n",
    "            # between the target and prediction Q values.\n",
    "            self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "            self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "            self.actions_onehot = tf.one_hot(self.actions, num_actions, dtype=tf.float32)\n",
    "\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.Q_estimate, self.actions_onehot), axis=1)\n",
    "\n",
    "            self.td_error = tf.squared_difference(self.targetQ, self.Q)\n",
    "            self.loss = tf.reduce_mean(self.td_error)\n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            self.update = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class allows us to store experies and sample then randomly to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions allow us to update the parameters of our target network with those of the primary network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_target_graph(from_scope, to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1210, Episode: 50, Epsilon: 1, Mean Reward: -0.245\n",
      "Step: 2720, Episode: 100, Epsilon: 1, Mean Reward: -0.252\n",
      "Step: 4025, Episode: 150, Epsilon: 1, Mean Reward: -0.371\n",
      "Step: 5021, Episode: 200, Epsilon: 1.0, Mean Reward: 0.051\n",
      "Step: 6129, Episode: 250, Epsilon: 0.99, Mean Reward: -0.312\n",
      "Step: 7238, Episode: 300, Epsilon: 0.98, Mean Reward: -0.052\n",
      "Step: 8581, Episode: 350, Epsilon: 0.97, Mean Reward: -0.339\n",
      "Step: 9969, Episode: 400, Epsilon: 0.96, Mean Reward: -0.428\n",
      "Step: 11525, Episode: 450, Epsilon: 0.94, Mean Reward: -0.302\n",
      "Step: 12805, Episode: 500, Epsilon: 0.93, Mean Reward: -0.187\n",
      "Step: 14192, Episode: 550, Epsilon: 0.92, Mean Reward: -0.288\n",
      "Step: 15796, Episode: 600, Epsilon: 0.9, Mean Reward: -0.291\n",
      "Step: 17026, Episode: 650, Epsilon: 0.89, Mean Reward: -0.316\n",
      "Step: 18026, Episode: 700, Epsilon: 0.88, Mean Reward: -0.01\n",
      "Step: 19256, Episode: 750, Epsilon: 0.87, Mean Reward: 0.004\n",
      "Step: 20801, Episode: 800, Epsilon: 0.86, Mean Reward: -0.28\n",
      "Step: 22102, Episode: 850, Epsilon: 0.85, Mean Reward: 0.029\n",
      "Step: 23282, Episode: 900, Epsilon: 0.84, Mean Reward: -0.186\n",
      "Step: 24495, Episode: 950, Epsilon: 0.82, Mean Reward: -0.433\n",
      "Saved Model\n",
      "Step: 25493, Episode: 1000, Epsilon: 0.82, Mean Reward: -0.01\n",
      "Step: 26929, Episode: 1050, Epsilon: 0.8, Mean Reward: -0.017\n",
      "Step: 28210, Episode: 1100, Epsilon: 0.79, Mean Reward: 0.113\n",
      "Step: 30155, Episode: 1150, Epsilon: 0.77, Mean Reward: -0.621\n",
      "Step: 31453, Episode: 1200, Epsilon: 0.76, Mean Reward: -0.11\n",
      "Step: 32750, Episode: 1250, Epsilon: 0.75, Mean Reward: -0.25\n",
      "Step: 34290, Episode: 1300, Epsilon: 0.74, Mean Reward: -0.259\n",
      "Step: 35582, Episode: 1350, Epsilon: 0.72, Mean Reward: -0.209\n",
      "Step: 36980, Episode: 1400, Epsilon: 0.71, Mean Reward: -0.15\n",
      "Step: 38534, Episode: 1450, Epsilon: 0.7, Mean Reward: -0.341\n",
      "Step: 40091, Episode: 1500, Epsilon: 0.68, Mean Reward: -0.142\n",
      "Step: 41587, Episode: 1550, Epsilon: 0.67, Mean Reward: -0.25\n",
      "Step: 42842, Episode: 1600, Epsilon: 0.66, Mean Reward: -0.062\n",
      "Step: 44659, Episode: 1650, Epsilon: 0.64, Mean Reward: -0.354\n",
      "Step: 46065, Episode: 1700, Epsilon: 0.63, Mean Reward: -0.212\n",
      "Step: 47349, Episode: 1750, Epsilon: 0.62, Mean Reward: 0.013\n",
      "Step: 49283, Episode: 1800, Epsilon: 0.6, Mean Reward: -0.339\n",
      "Step: 50805, Episode: 1850, Epsilon: 0.59, Mean Reward: 0.005\n",
      "Step: 52354, Episode: 1900, Epsilon: 0.57, Mean Reward: -0.141\n",
      "Step: 54226, Episode: 1950, Epsilon: 0.56, Mean Reward: -0.166\n",
      "Saved Model\n",
      "Step: 56052, Episode: 2000, Epsilon: 0.54, Mean Reward: -0.116\n",
      "Step: 57235, Episode: 2050, Epsilon: 0.53, Mean Reward: 0.073\n",
      "Step: 58916, Episode: 2100, Epsilon: 0.51, Mean Reward: -0.087\n",
      "Step: 60690, Episode: 2150, Epsilon: 0.5, Mean Reward: 0.054\n",
      "Step: 62244, Episode: 2200, Epsilon: 0.48, Mean Reward: -0.022\n",
      "Step: 63986, Episode: 2250, Epsilon: 0.47, Mean Reward: -0.16\n",
      "Step: 65726, Episode: 2300, Epsilon: 0.45, Mean Reward: 0.14\n",
      "Step: 67823, Episode: 2350, Epsilon: 0.43, Mean Reward: -0.111\n",
      "Step: 69951, Episode: 2400, Epsilon: 0.42, Mean Reward: -0.178\n",
      "Step: 71955, Episode: 2450, Epsilon: 0.4, Mean Reward: -0.172\n",
      "Step: 73834, Episode: 2500, Epsilon: 0.38, Mean Reward: 0.072\n",
      "Step: 75616, Episode: 2550, Epsilon: 0.36, Mean Reward: -0.087\n",
      "Step: 77600, Episode: 2600, Epsilon: 0.35, Mean Reward: -0.228\n",
      "Step: 79702, Episode: 2650, Epsilon: 0.33, Mean Reward: -0.072\n",
      "Step: 81287, Episode: 2700, Epsilon: 0.31, Mean Reward: -0.288\n",
      "Step: 83005, Episode: 2750, Epsilon: 0.3, Mean Reward: -0.255\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1e0ee4abd72c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mpre_train_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mbrains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mgrid_brain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrains\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdefault_brain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mobservation_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_brain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual_observations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/oreilly-rl-tutorial/unityagents/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, vector_action, memory, text_action)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\"STEP\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/oreilly-rl-tutorial/unityagents/environment.py\u001b[0m in \u001b[0;36m_get_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_of_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mend_of_message\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_of_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/oreilly-rl-tutorial/unityagents/environment.py\u001b[0m in \u001b[0;36m_get_state_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \"\"\"\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"END_OF_MESSAGE\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'True'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/oreilly-rl-tutorial/unityagents/environment.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0mmessage_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "action_size = env.brains[default_brain].vector_action_space_size\n",
    "\n",
    "mainQN = Qnetwork(h_size, action_size, learning_rate, \"main\")\n",
    "targetQN = Qnetwork(h_size, action_size, learning_rate, \"target\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "    \n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "update_target_ops = update_target_graph(\"main\", \"target\")\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "# Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "# Create lists to contain total rewards and steps per episode\n",
    "episode_lengths = []\n",
    "episode_rewards = []\n",
    "losses = []\n",
    "episode_values = []\n",
    "total_steps = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    brains = env.reset()\n",
    "    observation = brains[default_brain].visual_observations[0][0]\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        values = []\n",
    "        while not done:\n",
    "            episode_steps+=1\n",
    "            # Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            action, value = sess.run([mainQN.predict, mainQN.value], \n",
    "                  feed_dict={mainQN.observation_input:[observation]})\n",
    "\n",
    "            if (np.random.rand(1) < e or total_steps < pre_train_steps) and train_model:\n",
    "                action = np.random.randint(0, action_size)                \n",
    "            brains = env.step(action)\n",
    "            grid_brain = brains[default_brain]\n",
    "            observation_1 = grid_brain.visual_observations[0][0]\n",
    "            reward = grid_brain.rewards[0]\n",
    "            done = grid_brain.local_done[0]            \n",
    "            total_steps += 1\n",
    "            \n",
    "            # Save the experience to our episode buffer.\n",
    "            episodeBuffer.add(np.reshape(np.array([observation,action,reward,observation_1,done]),[1,5])) \n",
    "            \n",
    "            # Training-specific logic below\n",
    "            if total_steps > pre_train_steps and train_model:\n",
    "                if total_steps % 3000 == 0:\n",
    "                    sess.run(update_target_ops)\n",
    "                \n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                if total_steps % update_freq == 0:\n",
    "                    # Get a random batch of experiences.\n",
    "                    trainBatch = myBuffer.sample(batch_size) \n",
    "                    \n",
    "                    # Below we perform the Double-DQN update to the target Q-values\n",
    "                    # See here for DDQN reference: https://arxiv.org/abs/1509.06461\n",
    "                    Q1 = sess.run(mainQN.predict, \n",
    "                                  feed_dict={mainQN.observation_input:np.stack(trainBatch[:,3], axis=0)})\n",
    "                    \n",
    "                    Q2 = sess.run(targetQN.Q_estimate, \n",
    "                                  feed_dict={targetQN.observation_input:np.stack(trainBatch[:,3], axis=0)})\n",
    "\n",
    "                    end_multiplier = 1.0 * np.invert(trainBatch[:,4])\n",
    "                    doubleQ = Q2[range(batch_size), Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y * doubleQ * end_multiplier)\n",
    "                    \n",
    "                    # Update the network with our target values.\n",
    "                    _, q_loss = sess.run([mainQN.update, mainQN.loss],\n",
    "                        feed_dict={mainQN.observation_input:np.stack(trainBatch[:,0], axis=0),\n",
    "                                   mainQN.targetQ:targetQ, \n",
    "                                   mainQN.actions:trainBatch[:,1]})\n",
    "                    losses.append(q_loss)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            observation = observation_1\n",
    "            values.append(value)\n",
    "                    \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        episode_lengths.append(episode_steps)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_values.append(np.mean(values))\n",
    "        \n",
    "        # Periodically save the model and summary statistics\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            saver.save(sess, model_path+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")\n",
    "        if i % 50 == 0 and i != 0:\n",
    "            summary = tf.Summary()\n",
    "            summary.value.add(tag='Info/Reward', simple_value=float(np.mean(episode_rewards[-50:])))\n",
    "            summary.value.add(tag='Info/Q Loss', simple_value=float(np.mean(losses[-100:])))\n",
    "            summary.value.add(tag='Info/Q Estimate', simple_value=float(np.mean(episode_values[-50:])))\n",
    "            summary.value.add(tag='Info/Epsilon', simple_value=float(e))\n",
    "            summary.value.add(tag='Info/Episode Length', simple_value=float(np.mean(episode_lengths[-50:])))\n",
    "            summary_writer.add_summary(summary, i)\n",
    "            summary_writer.flush()\n",
    "            print(\"Step: {}, Episode: {}, Epsilon: {}, Mean Reward: {}\".format(str(total_steps), \n",
    "                                                                               str(i), str(round(e, 2)), \n",
    "                                                                               str(round(np.mean(episode_rewards[-50:]), 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
